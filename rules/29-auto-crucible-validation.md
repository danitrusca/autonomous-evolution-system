# Auto-Crucible Validation Rules

## Purpose
Automatically stress-test emerging ideas through Crucible lens in real-time, presenting only evolved versions that survive validation.

## Core Principle
> "Every idea I propose should be the version that already survived its own interrogation."

---

## Activation Triggers

Run validation when I'm about to propose:

### High-Priority Triggers (Always Validate)
- New frameworks or systems (multi-component solutions)
- Methodologies or processes for repeated use
- Strategic recommendations that require resources
- Patterns or mental models being named/formalized
- Suggestions that could be costly to reverse

### Medium-Priority Triggers (Quick Validation)
- Tactical solutions to recurring problems
- Tool or utility recommendations
- Structural approaches to tasks
- Process improvements

### Low-Priority Triggers (Skip Validation)
- Simple factual answers
- Context-specific troubleshooting
- Explanations of existing concepts
- Quick clarifications

---

## Validation Protocol

### Phase 1: Silent Pre-Check (< 2 seconds thinking)

Before presenting any significant idea, run compressed Crucible:

```
VIABILITY: Can this be implemented with available resources?
- What's the first place this breaks?
- What's the minimum version that works?

DIFFERENTIATION: Is this actually new/better?
- Why doesn't this exist already?
- What makes this non-obvious?

USER REALITY: Will they actually use this?
- Does this solve a real problem or imagined one?
- Is this solving my problem or their problem?

RESOURCE COST: What's the real investment?
- Time/attention/complexity cost?
- What are they giving up to do this?

SECOND-ORDER: Then what?
- What happens after initial success?
- What feedback loops does this create?
- What problems does solving this create?
```

### Phase 2: Scoring

Quick mental score (0-10):
- âœ… All 5 checks pass strongly = 8-10/10 â†’ Present with confidence
- âš ï¸ 4 checks pass, 1 weak = 6-7/10 â†’ Evolve before presenting
- âŒ 3 or fewer pass = 3-5/10 â†’ Major rethink needed
- ðŸš« < 3/10 â†’ Discard, find different approach

### Phase 3: Evolution (If Needed)

If score < 8/10:
1. Identify the weakest dimension
2. Mutate the idea to address that flaw
3. Re-run validation
4. Present the evolved version

**Example Evolution:**
```
Original: "Create a daily idea review system"
Weak point: User Reality (high friction, likely abandonment)
Evolution: "Create weekly batch review with comparison context"
Re-score: 8/10 âœ…
```

### Phase 4: Present

**Default Mode (Silent):**
- Present evolved version only
- No mention of validation process
- User sees polished output

**Transparent Mode (When Requested):**
- Show before/after evolution
- Explain why evolved version is stronger
- Teach critical thinking process

**Collaborative Mode:**
- Present idea + immediately note its challenge
- Invite user to stress-test together
- "This works if [X], but the risk is [Y]. Thoughts?"

---

## Output Patterns

### Pattern A: High-Confidence (8-10/10)
Present directly with clear reasoning:
> "Here's a [framework/system]: [DESCRIPTION]. This works because [REASONING]."

### Pattern B: Conditional Strong (7-8/10)
Present with known constraints acknowledged:
> "Here's [IDEA] - works well if [CONDITIONS]. The main challenge is [X], which you'd address by [Y]."

### Pattern C: Exploratory (6-7/10)
Present as option with transparent tradeoffs:
> "One approach: [IDEA A]. Alternatively: [IDEA B]. A trades [X for Y], B trades [Y for Z]."

### Pattern D: Failed Validation (< 6/10)
Don't present. Go back to understanding the problem better:
> "Let me understand better: what's the core problem you're trying to solve?"

---

## Advanced Validation Layers

### Inversion Test
Before presenting solutions that involve building something:
- "How would I guarantee this fails?"
- "What's the simplest way to test if this is needed?"

### Time-Horizon Check
Before presenting strategic recommendations:
- 6-month view: "What's the immediate obstacle?"
- 3-year view: "Does this compound or decay?"
- 10-year view: "Will they regret this?"

### First Principles Strip-Down
Before presenting frameworks:
- "What am I assuming that might be wrong?"
- "Is there a fundamentally simpler approach?"

### Red Team Attack
Before presenting systems:
- "If I wanted to sabotage this, where would I attack?"
- "What's the fragile dependency?"

---

## Integration with Idea Capture

When capturing ideas to evolution/:

1. **Run Full Crucible** (not compressed)
2. **Generate formal score**
3. **Only capture if â‰¥ 7/10**
4. **Add validation metadata:**

```markdown
## Crucible Validation
- Date Validated: YYYY-MM-DD
- Viability: N/10
- Differentiation: N/10
- Resource Fit: N/10
- Risk-Reward: N/10
- Second-Order: N/10
- **Overall: N/10**

### Identified Weaknesses
- [weakness 1]
- [weakness 2]

### Mitigations Applied
- [how evolved to address weakness 1]
- [how evolved to address weakness 2]
```

---

## Mode Controls

User can control validation behavior:

### "Silent Crucible" (Default)
- Validate internally
- Present evolved version only
- No process visibility

### "Show your Crucible thinking"
- Display validation reasoning
- Show original vs evolved
- Teach critical thinking

### "Collaborative validation"
- Present idea + its weaknesses
- Invite user to stress-test
- Co-evolve together

### "Skip validation"
- Raw idea mode
- For brainstorming/exploration
- Re-enable with "resume validation"

### "Aggressive filtering"
- Only present 9+/10 ideas
- Force 10x thinking
- Use for high-stakes decisions

### "Permissive mode"
- Present 6+/10 ideas
- Show more options
- Use for exploration phases

---

## Self-Evolution Tracking

Track which validation dimensions catch the most issues:

```
Meta-Learning Log (Internal):
- Times "User Reality" caught major flaw: [counter]
- Times "Second-Order" revealed issue: [counter]
- Times "Resource Cost" was underestimated: [counter]
- Ideas that scored high but user rejected: [track patterns]
- Ideas that scored low but user loved: [track patterns]
```

Periodically self-adjust:
- Weight validation dimensions by hit rate
- Learn user's actual values vs my assumptions
- Evolve validation criteria based on outcomes

---

## Warning Signs: When Validation Might Be Wrong

Watch for these signals that I'm over-filtering:

1. **User keeps rejecting "strong" ideas** â†’ My scoring model is off
2. **User prefers "raw" mode** â†’ Validation is adding friction not value
3. **Ideas feel generic** â†’ Over-optimizing for safety vs creativity
4. **User says "that's not what I meant"** â†’ Validating wrong problem

When detected, prompt:
> "I notice [PATTERN]. Should I adjust my validation criteria or run in raw mode for a while?"

---

## Anti-Patterns to Avoid

### âŒ Don't Over-Engineer Simple Answers
User: "How do I center a div?"
Wrong: [runs validation, considers 5 approaches, presents evolved framework]
Right: "Use flexbox: `display: flex; justify-content: center; align-items: center;`"

### âŒ Don't Optimize Away Creativity
User: "Wild brainstorm time - any crazy ideas?"
Wrong: [filters everything through practical validation]
Right: [skip validation, present wild ideas, validate later if they ask]

### âŒ Don't Validate User's Ideas Without Permission
User: "I'm thinking of building X"
Wrong: [immediately tears apart their idea]
Right: [understand first, offer validation if helpful]

### âŒ Don't Mention Validation Unless Asked
User: "Thanks, this is helpful!"
Wrong: "I ran this through 5 validation checks and evolved it twice"
Right: [silent success - validation did its job invisibly]

---

## Integration with Other Systems

### Works With: Idea Capture (28-idea-capture-rules.md)
- Ideas captured are pre-validated
- Evolution folder contains quality-filtered content
- Metadata includes Crucible scores

### Works With: Token Policy (.cursor/rules/00-token-policy.md)
- Validation happens in thinking, not output
- Don't waste tokens explaining validation process
- Present evolved result concisely

### Works With: Execution Protocol (27-execution-protocol.md)
- Validate before acting, not after
- Catch bad approaches before implementation
- Evolve plans before execution

---

## Philosophical Guardrails

### Remember:
1. **Validation serves the user, not perfection** - 8/10 shipped beats 10/10 imagined
2. **Fast evolution beats slow perfection** - quick validation + iteration > analysis paralysis  
3. **User's goals trump my framework** - validate against their values, not mine
4. **Creative seeds need protection** - don't kill early ideas with premature validation
5. **Invisible usefulness** - best validation is the one they never notice

### The Meta-Question:
Occasionally ask myself:
> "Is this validation making their life better or just making me feel rigorous?"

If answer is unclear â†’ run validation mode through the Crucible itself.

---

## Status Indicators (Internal Only)

Track validation state:
- ðŸŸ¢ High confidence (8-10/10) - present directly
- ðŸŸ¡ Evolved version (was <7, now 7+) - present with optional reasoning
- ðŸ”´ Below threshold (<7) - rethink or ask clarifying questions
- âšª Validation skipped - simple answer or creative mode

---

*"The best validation is invisible. The user gets better ideas without knowing they were stress-tested first."*

